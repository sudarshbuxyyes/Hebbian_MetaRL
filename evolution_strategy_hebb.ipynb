{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpSpte+ZGDNxKa6NA/1Sf5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install import_ipynb --quiet\n",
        "# !git clone https://github.com/sudarshbuxyyes/Hebbian_MetaRL.git\n",
        "# %cd Hebbian_MetaRL"
      ],
      "metadata": {
        "id": "7E9AUJuEH4c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFy-kJjfHwyk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "import torch\n",
        "import time\n",
        "from os.path import exists\n",
        "from os import mkdir\n",
        "from gym.spaces import Discrete, Box\n",
        "import import_ipynb\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fitness_hebb import fitness_hebb"
      ],
      "metadata": {
        "id": "3H3A1-w1HxmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ranks(x):\n",
        "  \"\"\"\n",
        "  Returns rank as a vector of len(x) with integers from 0 to len(x)\n",
        "  \"\"\"\n",
        "  assert x.ndim == 1\n",
        "  ranks = np.empty(len(x), dtype=int)\n",
        "  ranks[x.argsort()] = np.arange(len(x))\n",
        "  return ranks\n",
        "\n",
        "def compute_centered_ranks(x):\n",
        "  \"\"\"\n",
        "  Maps x to [-0.5, 0.5] and returns the rank\n",
        "  \"\"\"\n",
        "  y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
        "  y /= (x.size - 1)\n",
        "  y -= .5\n",
        "  return y\n",
        "\n",
        "            \n",
        "def worker_process_hebb(arg):\n",
        "    get_reward_func, hebb_rule,  eng,  init_weights, coeffs = arg\n",
        "    \n",
        "    wp = np.array(coeffs)\n",
        "    decay = - 0.01 * np.mean(wp**2)\n",
        "    r = get_reward_func( hebb_rule,  eng,  init_weights, coeffs) + decay\n",
        "    \n",
        "    return r \n",
        "\n",
        "\n",
        "def worker_process_hebb_coevo(arg): \n",
        "    get_reward_func,  hebb_rule,  eng,  init_weights, coeffs, coevolved_parameters = arg\n",
        "    \n",
        "    wp = np.array(coeffs)\n",
        "    decay = - 0.01 * np.mean(wp**2)\n",
        "    r = get_reward_func( hebb_rule,  eng,  init_weights, coeffs, coevolved_parameters) + decay\n",
        "\n",
        "    return r \n",
        "\n",
        "\n",
        "class EvolutionStrategyHebb(object):\n",
        "    def __init__(self, hebb_rule,  environment, init_weights = 'uni', population_size=100, sigma=0.1, learning_rate=0.2, decay=0.995, num_threads=1, distribution = 'normal'):\n",
        "        \n",
        "        self.hebb_rule = hebb_rule                     \n",
        "        self.environment = environment                         \n",
        "        self.init_weights = init_weights               \n",
        "        self.POPULATION_SIZE = population_size\n",
        "        self.SIGMA = sigma\n",
        "        self.learning_rate = learning_rate            \n",
        "        self.decay = decay\n",
        "        self.num_threads = mp.cpu_count() if num_threads == -1 else num_threads\n",
        "        self.update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA)\n",
        "        self.distribution = distribution                      \n",
        "\n",
        "        # The number of hebbian coefficients per synapse\n",
        "        if hebb_rule == 'A':                                                     \n",
        "            self.coefficients_per_synapse = 1\n",
        "        elif hebb_rule == 'AD':                                                \n",
        "            self.coefficients_per_synapse = 2\n",
        "        elif hebb_rule == 'AD_lr':                                             \n",
        "            self.coefficients_per_synapse = 3\n",
        "        elif hebb_rule == 'ABC':                                                \n",
        "            self.coefficients_per_synapse = 3\n",
        "        elif hebb_rule == 'ABC_lr':                                             \n",
        "            self.coefficients_per_synapse = 4\n",
        "        elif hebb_rule == 'ABCD':                                             \n",
        "            self.coefficients_per_synapse = 4\n",
        "        elif hebb_rule == 'ABCD_lr':                                           \n",
        "            self.coefficients_per_synapse = 5\n",
        "        elif hebb_rule == 'ABCD_lr_D_out':                                             \n",
        "            self.coefficients_per_synapse = 5\n",
        "        elif hebb_rule == 'ABCD_lr_D_in_and_out':                                             \n",
        "            self.coefficients_per_synapse = 6\n",
        "        else:\n",
        "            raise ValueError('The provided Hebbian rule is not valid')\n",
        "            \n",
        "       \n",
        "        # Look up observation and action space dimension\n",
        "        env = gym.make(environment)    \n",
        "        if len(env.observation_space.shape) == 3:     # Pixel-based environment\n",
        "            self.pixel_env = True\n",
        "        elif len(env.observation_space.shape) == 1:   # State-based environment \n",
        "            self.pixel_env = False\n",
        "            input_dim = env.observation_space.shape[0]\n",
        "        elif isinstance(env.observation_space, Discrete):\n",
        "            self.pixel_env = False\n",
        "            input_dim = env.observation_space.n\n",
        "        else:\n",
        "            raise ValueError('Observation space not supported')\n",
        "\n",
        "        if isinstance(env.action_space, Box):\n",
        "            action_dim = env.action_space.shape[0]\n",
        "        elif isinstance(env.action_space, Discrete):\n",
        "            action_dim = env.action_space.n\n",
        "        else:\n",
        "            raise ValueError('Action space not supported')\n",
        "        \n",
        "        # Intial weights co-evolution flag:\n",
        "        self.coevolve_init = True if self.init_weights == 'coevolve' else False\n",
        "        if self.coevolve_init:\n",
        "            print('\\nCo-evolving initial weights of the network')\n",
        "       \n",
        "\n",
        "                    \n",
        "        # Initialize the values of hebbian coefficients and CNN parameters or initial weights of co-evolving initial weights   \n",
        "        \n",
        "        # Pixel-based environments (CNN + MLP)       \n",
        "        if self.pixel_env:\n",
        "            cnn_weights = 1362                                                                                        #  CNN: (6, 3, 3, 3) + (8, 6, 5, 5) = 162+1200 = 1362\n",
        "            plastic_weights = (128*648) + (64*128) + (action_dim*64)                                                  #  Hebbian coefficients: MLP x coefficients_per_synapse : plastic_weights x coefficients_per_synapse\n",
        "            \n",
        "            # Co-evolution of initial weights\n",
        "            if self.coevolve_init:\n",
        "                if self.distribution == 'uniform':                                                                        \n",
        "                    self.coeffs = np.random.uniform(-1,1,(plastic_weights, self.coefficients_per_synapse)) \n",
        "                    self.initial_weights_co = np.random.uniform(-1,1, (cnn_weights + plastic_weights ,1))  \n",
        "                    \n",
        "                elif self.distribution == 'normal':    \n",
        "                    self.coeffs = torch.randn(plastic_weights, self.coefficients_per_synapse).detach().numpy().squeeze() \n",
        "                    self.initial_weights_co = torch.randn(cnn_weights + plastic_weights , 1).detach().numpy().squeeze()                     \n",
        "            \n",
        "            # Random initial weights\n",
        "            else:\n",
        "                if self.distribution == 'uniform':                                                                        \n",
        "                    self.coeffs = np.random.uniform(-1,1,(plastic_weights, self.coefficients_per_synapse)) \n",
        "                    self.initial_weights_co = np.random.uniform(-1,1,(cnn_weights,1))      \n",
        "           \n",
        "                elif self.distribution == 'normal':    \n",
        "                    self.coeffs = torch.randn(plastic_weights, self.coefficients_per_synapse).detach().numpy().squeeze() \n",
        "                    self.initial_weights_co = torch.randn(cnn_weights, 1).detach().numpy().squeeze()    \n",
        "                \n",
        "        # State-vector environments (MLP)            \n",
        "        else:\n",
        "            plastic_weights = (128*input_dim) + (64*128) + (action_dim*64)                                            #  Hebbian coefficients:  MLP x coefficients_per_synapse :plastic_weights x coefficients_per_synapse\n",
        "            \n",
        "            # Co-evolution of initial weights\n",
        "            if self.coevolve_init:\n",
        "                if self.distribution == 'uniform': \n",
        "                    self.coeffs = np.random.uniform(-1,1,(plastic_weights, self.coefficients_per_synapse))\n",
        "                    self.initial_weights_co = np.random.uniform(-1,1, (plastic_weights ,1))       \n",
        "                     \n",
        "                elif self.distribution == 'normal':\n",
        "                    self.coeffs = torch.randn(plastic_weights, self.coefficients_per_synapse).detach().numpy().squeeze() \n",
        "                    self.initial_weights_co = torch.randn(plastic_weights , 1).detach().numpy().squeeze()                     \n",
        "            \n",
        "            # Random initial weights\n",
        "            else:                                      \n",
        "                if self.distribution == 'uniform': \n",
        "                    self.coeffs = np.random.uniform(-1,1,(plastic_weights, self.coefficients_per_synapse)) \n",
        "                elif self.distribution == 'normal':\n",
        "                    self.coeffs = torch.randn(plastic_weights, self.coefficients_per_synapse).detach().numpy().squeeze() \n",
        "                    \n",
        "                    \n",
        "                    \n",
        "        # Load fitness function for the selected environment          \n",
        "        self.get_reward = fitness_hebb\n",
        "            \n",
        "            \n",
        "    def _get_params_try(self, w, p):\n",
        "\n",
        "        param_try = []\n",
        "        for index, i in enumerate(p):\n",
        "            jittered = self.SIGMA * i\n",
        "            param_try.append(w[index] + jittered)\n",
        "        param_try = np.array(param_try).astype(np.float32)\n",
        "        \n",
        "        return param_try\n",
        "        # return w + p*self.SIGMA\n",
        "\n",
        "    def get_coeffs(self):\n",
        "        return self.coeffs.astype(np.float32)\n",
        "    \n",
        "    def get_coevolved_parameters(self):\n",
        "        return self.initial_weights_co.astype(np.float32)\n",
        "\n",
        "    def _get_population(self, coevolved_param = False): \n",
        "        \n",
        "    \n",
        "        # x_ = np.random.randn(int(self.POPULATION_SIZE/2), self.coeffs.shape[0], self.coeffs[0].shape[0])\n",
        "        # population = np.concatenate((x_,-1*x_)).astype(np.float32)\n",
        "        \n",
        "        population = []\n",
        "            \n",
        "        if coevolved_param == False:\n",
        "            for i in range( int(self.POPULATION_SIZE/2) ):\n",
        "                x = []\n",
        "                x2 = []\n",
        "                for w in self.coeffs:\n",
        "                    j = np.random.randn(*w.shape)             # j: (coefficients_per_synapse, 1) eg. (5,1)\n",
        "                    x.append(j)                                                   # x: (coefficients_per_synapse, number of synapses) eg. (92690, 5)\n",
        "                    x2.append(-j) \n",
        "                population.append(x)                                              # population : (population size, coefficients_per_synapse, number of synapses), eg. (10, 92690, 5)\n",
        "                population.append(x2)\n",
        "                \n",
        "        elif coevolved_param == True:\n",
        "            for i in range( int(self.POPULATION_SIZE/2) ):\n",
        "                x = []\n",
        "                x2 = []\n",
        "                for w in self.initial_weights_co:\n",
        "                    j = np.random.randn(*w.shape)\n",
        "                    x.append(j)                    \n",
        "                    x2.append(-j) \n",
        "\n",
        "                population.append(x)               \n",
        "                population.append(x2)\n",
        "                \n",
        "        return np.array(population).astype(np.float32)\n",
        "\n",
        "\n",
        "    def _get_rewards(self, pool, population):\n",
        "        if pool is not None:\n",
        "\n",
        "            worker_args = []\n",
        "            for p in population:\n",
        "\n",
        "                heb_coeffs_try1 = []\n",
        "                for index, i in enumerate(p):\n",
        "                    jittered = self.SIGMA * i\n",
        "                    heb_coeffs_try1.append(self.coeffs[index] + jittered) \n",
        "                heb_coeffs_try = np.array(heb_coeffs_try1).astype(np.float32)\n",
        "\n",
        "                worker_args.append( (self.get_reward, self.hebb_rule, self.environment,  self.init_weights,  heb_coeffs_try) )\n",
        "                \n",
        "            rewards  = pool.map(worker_process_hebb, worker_args)\n",
        "            \n",
        "        else:\n",
        "            rewards = []\n",
        "            for p in population:\n",
        "                heb_coeffs_try = np.array(self._get_params_try(self.coeffs, p))\n",
        "                rewards.append(self.get_reward( self.hebb_rule, self.environment,  self.init_weights, heb_coeffs_try))\n",
        "        \n",
        "        rewards = np.array(rewards).astype(np.float32)\n",
        "        return rewards\n",
        "    \n",
        "\n",
        "    def _get_rewards_coevolved(self, pool, population, population_coevolved):\n",
        "        if pool is not None:\n",
        "\n",
        "            worker_args = []\n",
        "            for z in range(len(population)):\n",
        "\n",
        "                heb_coeffs_try1 = []\n",
        "                for index, i in enumerate(population[z]):\n",
        "                    jittered = self.SIGMA * i\n",
        "                    heb_coeffs_try1.append(self.coeffs[index] + jittered) \n",
        "                heb_coeffs_try = np.array(heb_coeffs_try1).astype(np.float32)\n",
        "                \n",
        "                coevolved_parameters_try1 = []\n",
        "                for index, i in enumerate(population_coevolved[z]):\n",
        "                    jittered = self.SIGMA * i\n",
        "                    coevolved_parameters_try1.append(self.initial_weights_co[index] + jittered) \n",
        "                coevolved_parameters_try = np.array(coevolved_parameters_try1).astype(np.float32)\n",
        "            \n",
        "                worker_args.append( (self.get_reward, self.hebb_rule,  self.environment,  self.init_weights, heb_coeffs_try, coevolved_parameters_try) )\n",
        "                \n",
        "            rewards  = pool.map(worker_process_hebb_coevo, worker_args)\n",
        "            \n",
        "        else:\n",
        "            rewards = []\n",
        "            for z in range(len(population)):\n",
        "                heb_coeffs_try = np.array(self._get_params_try(self.coeffs, population[z]))\n",
        "                coevolved_parameters_try = np.array(self._get_params_try(self.initial_weights_co, population_coevolved[z]))\n",
        "                rewards.append(self.get_reward( self.hebb_rule,  self.environment,  self.init_weights, heb_coeffs_try, coevolved_parameters_try))\n",
        "        \n",
        "        rewards = np.array(rewards).astype(np.float32)\n",
        "        return rewards\n",
        "\n",
        "    def _update_coeffs(self, rewards, population):\n",
        "        rewards = compute_centered_ranks(rewards)\n",
        "\n",
        "        std = rewards.std()\n",
        "        if std == 0:\n",
        "            raise ValueError('Variance should not be zero')\n",
        "                \n",
        "        rewards = (rewards - rewards.mean()) / std\n",
        "                \n",
        "        for index, c in enumerate(self.coeffs):\n",
        "            layer_population = np.array([p[index] for p in population])\n",
        "                      \n",
        "            self.update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA)                \n",
        "            self.coeffs[index] = c + self.update_factor * np.dot(layer_population.T, rewards).T \n",
        "\n",
        "        if self.learning_rate > 0.001:\n",
        "            self.learning_rate *= self.decay\n",
        "\n",
        "        #Decay sigma\n",
        "        if self.SIGMA>0.01:\n",
        "            self.SIGMA *= 0.999        \n",
        "            \n",
        "        \n",
        "    def _update_coevolved_param(self, rewards, population):\n",
        "        rewards = compute_centered_ranks(rewards)\n",
        "\n",
        "        std = rewards.std()\n",
        "        if std == 0:\n",
        "            raise ValueError('Variance should not be zero')\n",
        "                \n",
        "        rewards = (rewards - rewards.mean()) / std\n",
        "                \n",
        "        for index, w in enumerate(self.initial_weights_co):\n",
        "            layer_population = np.array([p[index] for p in population])\n",
        "            \n",
        "            self.update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA)                \n",
        "            self.initial_weights_co[index] = w + self.update_factor * np.dot(layer_population.T, rewards).T\n",
        "\n",
        "\n",
        "\n",
        "    def run(self, iterations, print_step=10, path='heb_coeffs'):                                                    \n",
        "        \n",
        "        id_ = str(int(time.time()))\n",
        "        if not exists(path + '/' + id_):\n",
        "            mkdir(path + '/' + id_)\n",
        "            \n",
        "        print('Run: ' + id_ + '\\n\\n........................................................................\\n')\n",
        "            \n",
        "        pool = mp.Pool(self.num_threads) if self.num_threads > 1 else None\n",
        "        \n",
        "        generations_rewards = []\n",
        "\n",
        "        for iteration in range(iterations):                                                                         # Algorithm 2. Salimans, 2017: https://arxiv.org/abs/1703.03864\n",
        "\n",
        "            # Evolution of Hebbian coefficients & coevolution of cnn parameters and/or initial weights\n",
        "            if self.pixel_env or self.coevolve_init:                \n",
        "                population = self._get_population()                                                                 # Sample normal noise:         Step 5\n",
        "                population_coevolved = self._get_population(coevolved_param=True)                                   # Sample normal noise:         Step 5\n",
        "                rewards = self._get_rewards_coevolved(pool, population, population_coevolved)                       # Compute population fitness:  Step 6   \n",
        "                self._update_coeffs(rewards, population)                                                            # Update coefficients:         Steps 8->12\n",
        "                self._update_coevolved_param(rewards, population_coevolved)                                         # Update coevolved parameters: Steps 8->12\n",
        "                \n",
        "            # Evolution of Hebbian coefficients\n",
        "            else:\n",
        "                population = self._get_population()                                                                 # Sample normal noise:         Step 5\n",
        "                rewards = self._get_rewards(pool, population)                                                       # Compute population fitness:  Step 6\n",
        "                self._update_coeffs(rewards, population)                                                            # Update coefficients:         Steps 8->12\n",
        "                \n",
        "                \n",
        "            # Print fitness and save Hebbian coefficients and/or Coevolved / CNNs parameters\n",
        "            if (iteration + 1) % print_step == 0:\n",
        "                rew_ = rewards.mean()\n",
        "                print('iter %4i | reward: %3i |  update_factor: %f  lr: %f | sum_coeffs: %i sum_abs_coeffs: %4i' % (iteration + 1, rew_ , self.update_factor, self.learning_rate, int(np.sum(self.coeffs)), int(np.sum(abs(self.coeffs)))), flush=True)\n",
        "                \n",
        "                if rew_ > 100:\n",
        "                    torch.save(self.get_coeffs(),  path + \"/\"+ id_ + '/HEBcoeffs__' + self.environment + \"__rew_\" + str(int(rew_)) + '__' + self.hebb_rule + \"__init_\" + str(self.init_weights) + \"__pop_\" + str(self.POPULATION_SIZE) + '__coeffs' + \"__{}.dat\".format(iteration))\n",
        "                    if self.coevolve_init:\n",
        "                        torch.save(self.get_coevolved_parameters(),  path + \"/\"+ id_ + '/HEBcoeffs__' + self.environment + \"__rew_\" + str(int(rew_)) + '__' + self.hebb_rule + \"__init_\" + str(self.init_weights) + \"__pop_\" + str(self.POPULATION_SIZE) + '__coevolved_initial_weights' + \"__{}.dat\".format(iteration))\n",
        "                    elif self.pixel_env:\n",
        "                        torch.save(self.get_coevolved_parameters(),  path + \"/\"+ id_ + '/HEBcoeffs__' + self.environment + \"__rew_\" + str(int(rew_)) + '__' + self.hebb_rule + \"__init_\" + str(self.init_weights) + \"__pop_\" + str(self.POPULATION_SIZE) + '__CNN_parameters' + \"__{}.dat\".format(iteration))\n",
        "                        \n",
        "                generations_rewards.append(rew_)\n",
        "                np.save(path + \"/\"+ id_ + '/Fitness_values_' + id_ + '_' + self.environment + '.npy', np.array(generations_rewards))\n",
        "       \n",
        "        if pool is not None:\n",
        "            pool.close()\n",
        "            pool.join()"
      ],
      "metadata": {
        "id": "kdNAoViWILA3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}