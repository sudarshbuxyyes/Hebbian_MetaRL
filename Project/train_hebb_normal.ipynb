{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "from os.path import join, exists\n",
    "from os import mkdir\n",
    "import gym\n",
    "import import_ipynb\n",
    "from gym.spaces import Discrete, Box\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import multiprocess as mp\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "gym.logger.set_level(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hebbian_update_ABCD_lr_D_in(heb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3):\n",
    "       \n",
    "        heb_offset = 0\n",
    "        ## Layer 1         \n",
    "        for i in range(weights1_2.shape[1]): \n",
    "            for j in range(weights1_2.shape[0]): \n",
    "                idx = (weights1_2.shape[0]-1)*i + i + j\n",
    "                weights1_2[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o0[i] * o1[j]\n",
    "                                                           + heb_coeffs[idx][1] * o0[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o1[j]  + heb_coeffs[idx][4])\n",
    "\n",
    "        heb_offset += weights1_2.shape[1] * weights1_2.shape[0]\n",
    "        # Layer 2\n",
    "        for i in range(weights2_3.shape[1]): \n",
    "            for j in range(weights2_3.shape[0]):\n",
    "                idx = heb_offset + (weights2_3.shape[0]-1)*i + i+j\n",
    "                weights2_3[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o1[i] * o2[j]\n",
    "                                                           + heb_coeffs[idx][1] * o1[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o2[j]  + heb_coeffs[idx][4])\n",
    "\n",
    "        heb_offset += weights2_3.shape[1] * weights2_3.shape[0]\n",
    "        # Layer 3\n",
    "        for i in range(weights3_4.shape[1]): \n",
    "            for j in range(weights3_4.shape[0]): \n",
    "                idx = heb_offset + (weights3_4.shape[0]-1)*i + i+j \n",
    "                weights3_4[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o2[i] * o3[j]\n",
    "                                                           + heb_coeffs[idx][1] * o2[i] \n",
    "                                                           + heb_coeffs[idx][2]         * o3[j]  + heb_coeffs[idx][4])\n",
    "                \n",
    "        return weights1_2, weights2_3, weights3_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_heb(nn.Module):\n",
    "    \"MLP, no bias\"\n",
    "    def __init__(self, input_space, action_space):\n",
    "        super(MLP_heb, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_space, 128, bias=False)\n",
    "        self.fc2 = nn.Linear(128, 64, bias=False)\n",
    "        self.fc3 = nn.Linear(64, action_space, bias=False)\n",
    "\n",
    "    def forward(self, ob):\n",
    "        import pickle\n",
    "        with open('ob.pkl', 'wb') as f:\n",
    "            pickle.dump(ob, f)\n",
    "        new_ob = ob[0][0]\n",
    "\n",
    "        if(type(new_ob) == np.float32):\n",
    "            new_ob = ob[0]\n",
    "        state = torch.as_tensor(new_ob).float().detach()\n",
    "        # print(state)\n",
    "        x1 = torch.tanh(self.fc1(state))   \n",
    "        x2 = torch.tanh(self.fc2(x1))\n",
    "        o = self.fc3(x2)  \n",
    "         \n",
    "        return state, x1, x2, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_hebb(hebb_rule : str, environment : str, init_weights = 'uni' , *evolved_parameters: List[np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an agent 'evolved_parameters' controlled by a Hebbian network in an environment 'environment' during a lifetime.\n",
    "    The initial weights are either co-evolved (if 'init_weights' == 'coevolve') along with the Hebbian coefficients or randomly sampled at each episode from the 'init_weights' distribution. \n",
    "    Subsequently the weights are updated following the hebbian update mechanism 'hebb_rule'.\n",
    "    Returns the episodic fitness of the agent.\n",
    "    \"\"\"\n",
    "    \n",
    "    def weights_init(m):\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            if init_weights == 'xa_uni':  \n",
    "                torch.nn.init.xavier_uniform(m.weight.data, 0.3)\n",
    "            elif init_weights == 'sparse':  \n",
    "                torch.nn.init.sparse_(m.weight.data, 0.8)\n",
    "            elif init_weights == 'uni':  \n",
    "                torch.nn.init.uniform_(m.weight.data, -0.1, 0.1)\n",
    "            elif init_weights == 'normal':  \n",
    "                torch.nn.init.normal_(m.weight.data, 0, 0.024)\n",
    "            elif init_weights == 'ka_uni':  \n",
    "                torch.nn.init.kaiming_uniform_(m.weight.data, 3)\n",
    "            elif init_weights == 'uni_big':\n",
    "                torch.nn.init.uniform_(m.weight.data, -1, 1)\n",
    "            elif init_weights == 'xa_uni_big':\n",
    "                torch.nn.init.xavier_uniform(m.weight.data)\n",
    "            elif init_weights == 'ones':\n",
    "                torch.nn.init.ones_(m.weight.data)\n",
    "            elif init_weights == 'zeros':\n",
    "                torch.nn.init.zeros_(m.weight.data)\n",
    "            elif init_weights == 'default':\n",
    "                pass\n",
    "            \n",
    "    # Unpack evolved parameters\n",
    "    try: \n",
    "        hebb_coeffs, initial_weights_co = evolved_parameters\n",
    "    except: \n",
    "        hebb_coeffs = evolved_parameters[0]\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "                    \n",
    "        # Load environment\n",
    "        try:\n",
    "            env = gym.make(environment, verbose = 0)\n",
    "        except:\n",
    "            env = gym.make(environment)\n",
    "            \n",
    "        # env.render()  # bullet envs\n",
    "        print(len(env.observation_space.shape))\n",
    "        print(isinstance(env.action_space, Box))\n",
    "        print(isinstance(env.action_space, Discrete))\n",
    "        \n",
    "        if len(env.observation_space.shape) == 1:   \n",
    "            pixel_env = False\n",
    "            input_dim = env.observation_space.shape[0]\n",
    "        elif len(env.observation_space.shape) == 0:   \n",
    "            pixel_env = False\n",
    "            input_dim = env.observation_space.n\n",
    "            \n",
    "        # Determine action space dimension\n",
    "\n",
    "        if isinstance(env.action_space, Box):\n",
    "            action_dim = env.action_space.shape[0]\n",
    "        elif isinstance(env.action_space, Discrete):\n",
    "            action_dim = env.action_space.n\n",
    "        else:\n",
    "            raise ValueError('Only Box and Discrete action spaces supported')\n",
    "        \n",
    "        # Initialise policy network\n",
    "        p = MLP_heb(input_dim, action_dim)          \n",
    "        \n",
    "        \n",
    "        # Initialise weights of the policy network with an specific distribution\n",
    "\n",
    "        # Randomly sample initial weights from chosen distribution\n",
    "        p.apply(weights_init)\n",
    "\n",
    "        p = p.float()\n",
    "        \n",
    "        weights1_2, weights2_3, weights3_4 = list(p.parameters())\n",
    "            \n",
    "        \n",
    "        # Convert weights to numpy so we can JIT them with Numba\n",
    "        weights1_2 = weights1_2.detach().numpy()\n",
    "        weights2_3 = weights2_3.detach().numpy()\n",
    "        weights3_4 = weights3_4.detach().numpy()\n",
    "        \n",
    "        observation = env.reset() \n",
    "              \n",
    "        # Normalize weights flag for non-bullet envs\n",
    "        normalised_weights = True\n",
    "\n",
    "        # Inner loop\n",
    "        neg_count = 0\n",
    "        rew_ep = 0\n",
    "        t = 0\n",
    "        while True:\n",
    "            \n",
    "            # For obaservation ∈ gym.spaces.Discrete, we one-hot encode the observation\n",
    "            # print(f'length:{env.length},gravity:{env.gravity}')\n",
    "            if isinstance(env.observation_space, Discrete): \n",
    "                observation = (observation == torch.arange(env.observation_space.n)).float()\n",
    "            \n",
    "            o0, o1, o2, o3 = p([observation])\n",
    "            o0 = o0.numpy()\n",
    "            o1 = o1.numpy()\n",
    "            o2 = o2.numpy()\n",
    "            \n",
    "            # Bounding the action space\n",
    "            if isinstance(env.action_space, Box):\n",
    "                action = o3.numpy()                        \n",
    "                action = np.clip(action, env.action_space.low, env.action_space.high)  \n",
    "            elif isinstance(env.action_space, Discrete):\n",
    "                action = np.argmax(o3).numpy()\n",
    "            o3 = o3.numpy()\n",
    "\n",
    "            \n",
    "            # Environment simulation step\n",
    "            observation, reward, done, dict = env.step(action)  \n",
    "            rew_ep += reward   \n",
    "                                       \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            t += 1\n",
    "            \n",
    "            #### Episodic/Intra-life hebbian update of the weights\n",
    "            if hebb_rule == 'ABCD_lr':\n",
    "                weights1_2, weights2_3, weights3_4 = hebbian_update_ABCD_lr_D_in(hebb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3)\n",
    "            else:\n",
    "                raise ValueError('The provided Hebbian rule is not valid')\n",
    "                \n",
    "\n",
    "            # Normalise weights per layer\n",
    "            (a, b, c) = (0, 1, 2) if not pixel_env else (2, 3, 4)\n",
    "            list(p.parameters())[a].data /= list(p.parameters())[a].__abs__().max()\n",
    "            list(p.parameters())[b].data /= list(p.parameters())[b].__abs__().max()\n",
    "            list(p.parameters())[c].data /= list(p.parameters())[c].__abs__().max()\n",
    "        \n",
    "            \n",
    "        env.close()\n",
    "\n",
    "    return rew_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_ranks(x):\n",
    "  \"\"\"\n",
    "  Returns rank as a vector of len(x) with integers from 0 to len(x)\n",
    "  \"\"\"\n",
    "  assert x.ndim == 1\n",
    "  ranks = np.empty(len(x), dtype=int)\n",
    "  ranks[x.argsort()] = np.arange(len(x))\n",
    "  return ranks\n",
    "\n",
    "def compute_centered_ranks(x):\n",
    "  \"\"\"\n",
    "  Maps x to [-0.5, 0.5] and returns the rank\n",
    "  \"\"\"\n",
    "  y = compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n",
    "  y /= (x.size - 1)\n",
    "  y -= .5\n",
    "  return y\n",
    "\n",
    "def worker_process_hebb(arg):\n",
    "    get_reward_func, hebb_rule,  eng,  init_weights, coeffs = arg\n",
    "    \n",
    "    wp = np.array(coeffs)\n",
    "    decay = - 0.01 * np.mean(wp**2)\n",
    "    r = get_reward_func( hebb_rule,  eng,  init_weights, coeffs) + decay\n",
    "    \n",
    "    return r \n",
    "\n",
    "def worker_process_hebb_coevo(arg): \n",
    "    get_reward_func,  hebb_rule,  eng,  init_weights, coeffs, coevolved_parameters = arg\n",
    "    \n",
    "    wp = np.array(coeffs)\n",
    "    decay = - 0.01 * np.mean(wp**2)\n",
    "    r = get_reward_func( hebb_rule,  eng,  init_weights, coeffs, coevolved_parameters) + decay\n",
    "\n",
    "    return r \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "def worker_process_hebb_coevo(arg): \n",
    "    get_reward_func,  hebb_rule,  eng,  init_weights, coeffs, coevolved_parameters = arg\n",
    "    \n",
    "    wp = np.array(coeffs)\n",
    "    decay = - 0.01 * np.mean(wp**2)\n",
    "    r = get_reward_func( hebb_rule,  eng,  init_weights, coeffs, coevolved_parameters) + decay\n",
    "\n",
    "    return r \n",
    "class EvolutionStrategyHebb(object):\n",
    "    def __init__(self, hebb_rule,  environment, init_weights = 'uni', population_size=100, sigma=0.1, learning_rate=0.2, decay=0.995, num_threads=1, distribution = 'normal'):\n",
    "        \n",
    "        self.hebb_rule = hebb_rule                     \n",
    "        self.environment = environment                         \n",
    "        self.init_weights = init_weights               \n",
    "        self.POPULATION_SIZE = population_size\n",
    "        self.SIGMA = sigma\n",
    "        self.learning_rate = learning_rate            \n",
    "        self.decay = decay\n",
    "        self.num_threads = mp.cpu_count() if num_threads == -1 else num_threads\n",
    "        self.update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA)\n",
    "        self.distribution = distribution                      \n",
    "\n",
    "        # The number of hebbian coefficients per synapse\n",
    "        if hebb_rule == 'ABCD_lr':                                           \n",
    "            self.coefficients_per_synapse = 5\n",
    "        else:\n",
    "            raise ValueError('The provided Hebbian rule is not valid')\n",
    "            \n",
    "       \n",
    "        # Look up observation and action space dimension\n",
    "        env = gym.make(environment)    \n",
    "\n",
    "        if len(env.observation_space.shape) == 1:   # State-based environment \n",
    "            self.pixel_env = False\n",
    "            input_dim = env.observation_space.shape[0]\n",
    "        elif isinstance(env.observation_space, Discrete):\n",
    "            self.pixel_env = False\n",
    "            input_dim = env.observation_space.n\n",
    "        else:\n",
    "            raise ValueError('Observation space not supported')\n",
    "\n",
    "        if isinstance(env.action_space, Box):\n",
    "            action_dim = env.action_space.shape[0]\n",
    "        elif isinstance(env.action_space, Discrete):\n",
    "            action_dim = env.action_space.n\n",
    "        else:\n",
    "            raise ValueError('Action space not supported')\n",
    "                    \n",
    "        # Initialize the values of hebbian coefficients and CNN parameters or initial weights of co-evolving initial weights   \n",
    "                  \n",
    "        # State-vector environments (MLP)            \n",
    "        plastic_weights = (128*input_dim) + (64*128) + (action_dim*64)                                            #  Hebbian coefficients:  MLP x coefficients_per_synapse :plastic_weights x coefficients_per_synapse\n",
    "        \n",
    "        # Co-evolution of initial weights                \n",
    "        \n",
    "        # Random initial weights\n",
    "        if self.distribution == 'uniform': \n",
    "            self.coeffs = np.random.uniform(-1,1,(plastic_weights, self.coefficients_per_synapse)) \n",
    "        elif self.distribution == 'normal':\n",
    "            self.coeffs = torch.randn(plastic_weights, self.coefficients_per_synapse).detach().numpy().squeeze() \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        # Load fitness function for the selected environment          \n",
    "        self.get_reward = fitness_hebb\n",
    "            \n",
    "            \n",
    "    def _get_params_try(self, w, p):\n",
    "\n",
    "        param_try = []\n",
    "        for index, i in enumerate(p):\n",
    "            jittered = self.SIGMA * i\n",
    "            param_try.append(w[index] + jittered)\n",
    "        param_try = np.array(param_try).astype(np.float32)\n",
    "        \n",
    "        return param_try\n",
    "        # return w + p*self.SIGMA\n",
    "\n",
    "    def get_coeffs(self):\n",
    "        return self.coeffs.astype(np.float32)\n",
    "\n",
    "    def _get_population(self, coevolved_param = False): \n",
    "        \n",
    "        # x_ = np.random.randn(int(self.POPULATION_SIZE/2), self.coeffs.shape[0], self.coeffs[0].shape[0])\n",
    "        # population = np.concatenate((x_,-1*x_)).astype(np.float32)\n",
    "        \n",
    "        population = []\n",
    "            \n",
    "        if coevolved_param == False:\n",
    "            for i in range( int(self.POPULATION_SIZE/2) ):\n",
    "                x = []\n",
    "                x2 = []\n",
    "                for w in self.coeffs:\n",
    "                    j = np.random.randn(*w.shape)             # j: (coefficients_per_synapse, 1) eg. (5,1)\n",
    "                    x.append(j)                                                   # x: (coefficients_per_synapse, number of synapses) eg. (92690, 5)\n",
    "                    x2.append(-j) \n",
    "                population.append(x)                                              # population : (population size, coefficients_per_synapse, number of synapses), eg. (10, 92690, 5)\n",
    "                population.append(x2)\n",
    "                \n",
    "        elif coevolved_param == True:\n",
    "            for i in range( int(self.POPULATION_SIZE/2) ):\n",
    "                x = []\n",
    "                x2 = []\n",
    "                for w in self.initial_weights_co:\n",
    "                    j = np.random.randn(*w.shape)\n",
    "                    x.append(j)                    \n",
    "                    x2.append(-j) \n",
    "\n",
    "                population.append(x)               \n",
    "                population.append(x2)\n",
    "                \n",
    "        return np.array(population).astype(np.float32)\n",
    "\n",
    "\n",
    "    def _get_rewards(self, pool, population):\n",
    "        if pool is not None:\n",
    "\n",
    "            worker_args = []\n",
    "            for p in population:\n",
    "\n",
    "                heb_coeffs_try1 = []\n",
    "                for index, i in enumerate(p):\n",
    "                    jittered = self.SIGMA * i\n",
    "                    heb_coeffs_try1.append(self.coeffs[index] + jittered) \n",
    "                heb_coeffs_try = np.array(heb_coeffs_try1).astype(np.float32)\n",
    "\n",
    "                worker_args.append( (self.get_reward, self.hebb_rule, self.environment,  self.init_weights,  heb_coeffs_try) )\n",
    "                \n",
    "            rewards  = pool.map(worker_process_hebb, worker_args)\n",
    "            \n",
    "        else:\n",
    "            rewards = []\n",
    "            for p in population:\n",
    "                heb_coeffs_try = np.array(self._get_params_try(self.coeffs, p))\n",
    "                rewards.append(self.get_reward( self.hebb_rule, self.environment,  self.init_weights, heb_coeffs_try))\n",
    "        \n",
    "        rewards = np.array(rewards).astype(np.float32)\n",
    "        return rewards\n",
    "    \n",
    "\n",
    "\n",
    "    def _update_coeffs(self, rewards, population):\n",
    "        rewards = compute_centered_ranks(rewards)\n",
    "\n",
    "        std = rewards.std()\n",
    "        if std == 0:\n",
    "            raise ValueError('Variance should not be zero')\n",
    "                \n",
    "        rewards = (rewards - rewards.mean()) / std\n",
    "                \n",
    "        for index, c in enumerate(self.coeffs):\n",
    "            layer_population = np.array([p[index] for p in population])\n",
    "                      \n",
    "            self.update_factor = self.learning_rate / (self.POPULATION_SIZE * self.SIGMA)                \n",
    "            self.coeffs[index] = c + self.update_factor * np.dot(layer_population.T, rewards).T \n",
    "\n",
    "        if self.learning_rate > 0.001:\n",
    "            self.learning_rate *= self.decay\n",
    "\n",
    "        #Decay sigma\n",
    "        if self.SIGMA>0.01:\n",
    "            self.SIGMA *= 0.999        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def run(self, iterations, print_step=10, path='heb_coeffs'):                                                    \n",
    "        \n",
    "        id_ = str(int(time.time()))\n",
    "        if not exists(path + '/' + id_):\n",
    "            mkdir(path + '/' + id_)\n",
    "            \n",
    "        print('Run: ' + id_ + '\\n\\n........................................................................\\n')\n",
    "            \n",
    "        pool = mp.Pool(self.num_threads) if self.num_threads > 1 else None\n",
    "        \n",
    "        generations_rewards = []\n",
    "\n",
    "        for iteration in range(iterations):                                                                         # Algorithm 2. Salimans, 2017: https://arxiv.org/abs/1703.03864\n",
    "\n",
    "            population = self._get_population()                                                                 # Sample normal noise:         Step 5\n",
    "            rewards = self._get_rewards(pool, population)                                                       # Compute population fitness:  Step 6\n",
    "            self._update_coeffs(rewards, population)                                                            # Update coefficients:         Steps 8->12\n",
    "                \n",
    "                \n",
    "            # Print fitness and save Hebbian coefficients and/or Coevolved / CNNs parameters\n",
    "            if (iteration + 1) % print_step == 0:\n",
    "                rew_ = rewards.mean()\n",
    "                print('iter %4i | reward: %3i |  update_factor: %f  lr: %f | sum_coeffs: %i sum_abs_coeffs: %4i' % (iteration + 1, rew_ , self.update_factor, self.learning_rate, int(np.sum(self.coeffs)), int(np.sum(abs(self.coeffs)))), flush=True)\n",
    "                \n",
    "                if rew_ > 100:\n",
    "                    torch.save(self.get_coeffs(),  path + \"/\"+ id_ + '/HEBcoeffs__' + self.environment + \"__rew_\" + str(int(rew_)) + '__' + self.hebb_rule + \"__init_\" + str(self.init_weights) + \"__pop_\" + str(self.POPULATION_SIZE) + '__coeffs' + \"__{}.dat\".format(iteration))\n",
    "                        \n",
    "                generations_rewards.append(rew_)\n",
    "                np.save(path + \"/\"+ id_ + '/Fitness_values_' + id_ + '_' + self.environment + '.npy', np.array(generations_rewards))\n",
    "       \n",
    "        if pool is not None:\n",
    "            pool.close()\n",
    "            pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "........................................................................\n",
      "\n",
      "Initilisating Hebbian ES for CartPole-v1 with ABCD_lr Hebbian rule\n",
      "\n",
      "\n",
      "........................................................................\n",
      "\n",
      " ♪┏(°.°)┛┗(°.°)┓ Starting Evolution ┗(°.°)┛┏(°.°)┓ ♪ \n",
      "\n",
      "Run: 1682844467\n",
      "\n",
      "........................................................................\n",
      "\n",
      "111\n",
      "\n",
      "1\n",
      "FalseFalse\n",
      "FalseFalse\n",
      "\n",
      "\n",
      "\n",
      "TrueTrueTrue1True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1False\n",
      "\n",
      "FalseTrue\n",
      "1\n",
      "\n",
      "True\n",
      "1False\n",
      "\n",
      "TrueFalse\n",
      "\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "1True\n",
      "False\n",
      "True\n",
      "\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False1\n",
      "\n",
      "FalseTrue\n",
      "\n",
      "True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEvolution took: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mint\u001b[39m(toc\u001b[39m-\u001b[39mtic), \u001b[39m'\u001b[39m\u001b[39m seconds\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m     main(sys\u001b[39m.\u001b[39;49margv)\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m ♪┏(°.°)┛┗(°.°)┓ Starting Evolution ┗(°.°)┛┏(°.°)┓ ♪ \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 29\u001b[0m es\u001b[39m.\u001b[39;49mrun(generations, print_step\u001b[39m=\u001b[39;49mprint_every, path\u001b[39m=\u001b[39;49mfolder)\n\u001b[1;32m     30\u001b[0m toc \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEvolution took: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mint\u001b[39m(toc\u001b[39m-\u001b[39mtic), \u001b[39m'\u001b[39m\u001b[39m seconds\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 221\u001b[0m, in \u001b[0;36mEvolutionStrategyHebb.run\u001b[0;34m(self, iterations, print_step, path)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iterations):                                                                         \u001b[39m# Algorithm 2. Salimans, 2017: https://arxiv.org/abs/1703.03864\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     population \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_population()                                                                 \u001b[39m# Sample normal noise:         Step 5\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_rewards(pool, population)                                                       \u001b[39m# Compute population fitness:  Step 6\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_coeffs(rewards, population)                                                            \u001b[39m# Update coefficients:         Steps 8->12\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[39m# Print fitness and save Hebbian coefficients and/or Coevolved / CNNs parameters\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 168\u001b[0m, in \u001b[0;36mEvolutionStrategyHebb._get_rewards\u001b[0;34m(self, pool, population)\u001b[0m\n\u001b[1;32m    164\u001b[0m         heb_coeffs_try \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(heb_coeffs_try1)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    166\u001b[0m         worker_args\u001b[39m.\u001b[39mappend( (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhebb_rule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment,  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_weights,  heb_coeffs_try) )\n\u001b[0;32m--> 168\u001b[0m     rewards  \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(worker_process_hebb, worker_args)\n\u001b[1;32m    170\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     rewards \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/mtl/lib/python3.9/site-packages/multiprocess/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/mtl/lib/python3.9/site-packages/multiprocess/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mtl/lib/python3.9/site-packages/multiprocess/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/mtl/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/mtl/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n",
      "1\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "\n",
    "    environment = 'CartPole-v1'\n",
    "    hebb_rule = 'ABCD_lr' # A, AD, AD_lr, ABC, ABC_lr, ABCD, ABCD_lr, ABCD_lr_D_out, ABCD_lr_D_in_and_out\n",
    "    popsize = 200 # Population size # 200\n",
    "    lr = 0.2 # Learning rate\n",
    "    decay = 0.995 # Learning rate decay\n",
    "    sigma = 0.1 # Modulates the amount of noise used to populate each new generation\n",
    "    init_weights = 'uni' # The distribution used to sample random weights from at each episode: uni, normal, default, xa_uni, sparse, ka_uni or coevolve to co-evolve the intial weights\n",
    "    print_every = 1 # Print and save every N steps\n",
    "    generations = 50 # Number of generations that the ES will run # 50\n",
    "    threads = -1 # Number of threads used to run evolution in parallel: -1 uses all threads available\n",
    "    folder = 'heb_coeffs' # Folder to store the evolved Hebbian coefficients\n",
    "    distribution = 'normal' # Sampling distribution for initialize the Hebbian coefficients: normal, uniform\n",
    "\n",
    "\n",
    "    if not exists(folder):\n",
    "        mkdir(folder)\n",
    "        \n",
    "    # Initialise the EvolutionStrategy class\n",
    "    print('\\n\\n........................................................................')\n",
    "    print('\\nInitilisating Hebbian ES for ' + environment + ' with ' + hebb_rule + ' Hebbian rule\\n')\n",
    "    es = EvolutionStrategyHebb(hebb_rule, environment, init_weights, population_size=popsize, sigma=sigma, learning_rate=lr, decay=decay, num_threads=threads, distribution=distribution)\n",
    "    \n",
    "    # Start the evolution\n",
    "    print('\\n........................................................................')\n",
    "    print('\\n ♪┏(°.°)┛┗(°.°)┓ Starting Evolution ┗(°.°)┛┏(°.°)┓ ♪ \\n')\n",
    "    tic = time.time()\n",
    "    es.run(generations, print_step=print_every, path=folder)\n",
    "    toc = time.time()\n",
    "    print('\\nEvolution took: ', int(toc-tic), ' seconds\\n')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
