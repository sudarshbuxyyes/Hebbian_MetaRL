{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-04-30T15:01:32.694519Z","iopub.status.busy":"2023-04-30T15:01:32.694101Z","iopub.status.idle":"2023-04-30T15:02:51.125229Z","shell.execute_reply":"2023-04-30T15:02:51.123703Z","shell.execute_reply.started":"2023-04-30T15:01:32.694482Z"},"id":"d8217774-d277-45f5-8cf8-d5132d0ca793","outputId":"eea6476b-f76b-4ac6-e6d1-9a67a6db2df4","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: setuptools==65.5.0 in /Users/joykirat/miniconda3/envs/mtl/lib/python3.9/site-packages (65.5.0)\n","fatal: destination path 'aiagentarch' already exists and is not an empty directory.\n","Requirement already satisfied: pyglet==1.5.27 in /Users/joykirat/miniconda3/envs/mtl/lib/python3.9/site-packages (1.5.27)\n","/Users/joykirat/Downloads/sem8/MTL/Hebbian_MetaRL/Project/aiagentarch\n"]}],"source":["!pip install --upgrade setuptools==65.5.0\n","!pip install --quiet stable_baselines3\n","!pip install --quiet import_ipynb\n","!git clone https://github.com/gmshroff/aiagentarch.git\n","!pip install pyglet==1.5.27\n","%cd aiagentarch"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:51.127721Z","iopub.status.busy":"2023-04-30T15:02:51.127302Z","iopub.status.idle":"2023-04-30T15:02:55.770420Z","shell.execute_reply":"2023-04-30T15:02:55.769201Z","shell.execute_reply.started":"2023-04-30T15:02:51.127681Z"},"id":"d0e8289f-e2f8-43d4-bfd4-b0913edf0f29","trusted":true},"outputs":[],"source":["import gym\n","from gym import spaces\n","from gym import Env\n","import random\n","import numpy as np\n","from threading import Thread\n","import threading\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.vec_env import DummyVecEnv,VecFrameStack\n","from stable_baselines3.common.monitor import Monitor as Mon\n","from stable_baselines3.common.vec_env import StackedObservations\n","import random\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:55.773696Z","iopub.status.busy":"2023-04-30T15:02:55.772809Z","iopub.status.idle":"2023-04-30T15:02:55.912401Z","shell.execute_reply":"2023-04-30T15:02:55.911001Z","shell.execute_reply.started":"2023-04-30T15:02:55.773642Z"},"id":"dd282224-20ee-4bb0-85e8-82a65cb6538a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["importing Jupyter notebook from aiagentbase.ipynb\n","importing Jupyter notebook from cartpole_tasks.ipynb\n"]}],"source":["import import_ipynb\n","from aiagentbase import AIAgent,Controller,Memory,Perception,Actor\n","from cartpole_tasks import CartPoleEnv\n","from gym.spaces import Discrete, Box\n","import torch\n","from typing import List, Any\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:55.917468Z","iopub.status.busy":"2023-04-30T15:02:55.916201Z","iopub.status.idle":"2023-04-30T15:02:55.931892Z","shell.execute_reply":"2023-04-30T15:02:55.929770Z","shell.execute_reply.started":"2023-04-30T15:02:55.917409Z"},"id":"1f55b243-bf59-4ac8-93c8-e001b25737b6","tags":[],"trusted":true},"outputs":[],"source":["import multiprocess as mp"]},{"cell_type":"markdown","metadata":{"id":"67e0c3d3-6686-4413-b5be-f9bd31ad73ab"},"source":["### Agent-based RL in Simple Worlds"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:55.934845Z","iopub.status.busy":"2023-04-30T15:02:55.934057Z","iopub.status.idle":"2023-04-30T15:02:55.944792Z","shell.execute_reply":"2023-04-30T15:02:55.942024Z","shell.execute_reply.started":"2023-04-30T15:02:55.934789Z"},"trusted":true},"outputs":[],"source":["# environment = 'CartPole-v1'\n","# hebb_rule = 'ABCD_lr' # A, AD, AD_lr, ABC, ABC_lr, ABCD, ABCD_lr, ABCD_lr_D_out, ABCD_lr_D_in_and_out\n","# popsize = 10 # Population size # 200\n","# learning_rate = 0.2 # Learning rate\n","# decay = 0.995 # Learning rate decay\n","# SIGMA = 0.1 # Modulates the amount of noise used to populate each new generation\n","# init_weights = 'uni' # The distribution used to sample random weights from at each episode: uni, normal, default, xa_uni, sparse, ka_uni or coevolve to co-evolve the intial weights\n","# print_every = 1 # Print and save every N steps\n","# generations = 5 # Number of generations that the ES will run # 50\n","# threads = -1 # Number of threads used to run evolution in parallel: -1 uses all threads available\n","# folder = 'heb_coeffs' # Folder to store the evolved Hebbian coefficients\n","# distribution = 'normal' # Sampling distribution for initialize the Hebbian coefficients: normal, uniform\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:55.947431Z","iopub.status.busy":"2023-04-30T15:02:55.946806Z","iopub.status.idle":"2023-04-30T15:02:55.963924Z","shell.execute_reply":"2023-04-30T15:02:55.961649Z","shell.execute_reply.started":"2023-04-30T15:02:55.947377Z"},"trusted":true},"outputs":[],"source":["args = {\n","    'environment' : 'CartPole-v1',\n","    'hebb_rule' : 'ABCD_lr', # A, AD, AD_lr, ABC, ABC_lr, ABCD, ABCD_lr, ABCD_lr_D_out, ABCD_lr_D_in_and_out\n","    'popsize' : 10, # Population size # 200\n","    'learning_rate' : 0.2, # Learning rate\n","    'decay' : 0.995, # Learning rate decay\n","    'sigma' : 0.1, # Modulates the amount of noise used to populate each new generation\n","    'init_weights' :'uni', # The distribution used to sample random weights from at each episode: uni, normal, default, xa_uni, sparse, ka_uni or coevolve to co-evolve the intial weights\n","    'print_every' : 1, # Print and save every N steps\n","    'generations' : 5, # Number of generations that the ES will run # 50\n","    'threads' : -1, # Number of threads used to run evolution in parallel: -1 uses all threads available\n","    'folder' : 'heb_coeffs', # Folder to store the evolved Hebbian coefficients\n","    'distribution' : 'normal', # Sampling distribution for initialize the Hebbian coefficients: normal, uniform\n","}"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:55.966738Z","iopub.status.busy":"2023-04-30T15:02:55.966191Z","iopub.status.idle":"2023-04-30T15:02:56.024585Z","shell.execute_reply":"2023-04-30T15:02:56.022212Z","shell.execute_reply.started":"2023-04-30T15:02:55.966691Z"},"trusted":true},"outputs":[],"source":["num_threads = mp.cpu_count() if args['threads'] == -1 else args['threads']\n","update_factor = args['learning_rate'] / (args['popsize'] * args['sigma'])\n","distribution = args['distribution']                      \n","\n","# The number of hebbian coefficients per synapse\n","if args['hebb_rule'] == 'ABCD_lr':                                           \n","    coefficients_per_synapse = 5\n","else:\n","    raise ValueError('The provided Hebbian rule is not valid')\n","    \n","\n","# Look up observation and action space dimension\n","env = CartPoleEnv()\n","\n","if len(env.observation_space.shape) == 1:   # State-based environment \n","    pixel_env = False\n","    input_dim = env.observation_space.shape[0]\n","elif isinstance(env.observation_space, Discrete):\n","    pixel_env = False\n","    input_dim = env.observation_space.n\n","else:\n","    raise ValueError('Observation space not supported')\n","\n","if isinstance(env.action_space, Box):\n","    action_dim = env.action_space.shape[0]\n","elif isinstance(env.action_space, Discrete):\n","    action_dim = env.action_space.n\n","else:\n","    raise ValueError('Action space not supported')\n","            \n","# Initialize the values of hebbian coefficients and CNN parameters or initial weights of co-evolving initial weights   \n","            \n","# State-vector environments (MLP)            \n","plastic_weights = (128*input_dim) + (64*128) + (action_dim*64)                                            #  Hebbian coefficients:  MLP x coefficients_per_synapse :plastic_weights x coefficients_per_synapse\n","\n","# Co-evolution of initial weights                \n","\n","# Random initial weights\n","if distribution == 'uniform': \n","    coeffs = np.random.uniform(-1,1,(plastic_weights, coefficients_per_synapse)) \n","elif distribution == 'normal':\n","    coeffs = torch.randn(plastic_weights, coefficients_per_synapse).detach().numpy().squeeze() "]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:56.027450Z","iopub.status.busy":"2023-04-30T15:02:56.026911Z","iopub.status.idle":"2023-04-30T15:02:56.039506Z","shell.execute_reply":"2023-04-30T15:02:56.037792Z","shell.execute_reply.started":"2023-04-30T15:02:56.027401Z"},"trusted":true},"outputs":[{"data":{"text/plain":["0.5"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["env.length"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:56.042355Z","iopub.status.busy":"2023-04-30T15:02:56.041874Z","iopub.status.idle":"2023-04-30T15:02:56.054163Z","shell.execute_reply":"2023-04-30T15:02:56.052372Z","shell.execute_reply.started":"2023-04-30T15:02:56.042315Z"},"trusted":true},"outputs":[],"source":["class MLP_heb(nn.Module):\n","    \"MLP, no bias\"\n","    def __init__(self, input_space, action_space):\n","        super(MLP_heb, self).__init__()\n","\n","        self.fc1 = nn.Linear(input_space, 128, bias=False)\n","        self.fc2 = nn.Linear(128, 64, bias=False)\n","        self.fc3 = nn.Linear(64, action_space, bias=False)\n","\n","    def forward(self, ob):\n","        import pickle\n","        with open('ob.pkl', 'wb') as f:\n","            pickle.dump(ob, f)\n","        new_ob = ob[0][0]\n","\n","        if(type(new_ob) == np.float32):\n","            new_ob = ob[0]\n","        state = torch.as_tensor(new_ob).float().detach()\n","        # print(state)\n","        x1 = torch.tanh(self.fc1(state))   \n","        x2 = torch.tanh(self.fc2(x1))\n","        o = self.fc3(x2)  \n","         \n","        return state, x1, x2, o    "]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:02:56.062277Z","iopub.status.busy":"2023-04-30T15:02:56.061237Z","iopub.status.idle":"2023-04-30T15:02:56.083165Z","shell.execute_reply":"2023-04-30T15:02:56.081705Z","shell.execute_reply.started":"2023-04-30T15:02:56.062224Z"},"trusted":true},"outputs":[],"source":["def hebbian_update_ABCD_lr_D_in(heb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3):\n","       \n","        heb_offset = 0\n","        ## Layer 1         \n","        for i in range(weights1_2.shape[1]): \n","            for j in range(weights1_2.shape[0]): \n","                idx = (weights1_2.shape[0]-1)*i + i + j\n","                weights1_2[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o0[i] * o1[j]\n","                                                           + heb_coeffs[idx][1] * o0[i] \n","                                                           + heb_coeffs[idx][2]         * o1[j]  + heb_coeffs[idx][4])\n","\n","        heb_offset += weights1_2.shape[1] * weights1_2.shape[0]\n","        # Layer 2\n","        for i in range(weights2_3.shape[1]): \n","            for j in range(weights2_3.shape[0]):\n","                idx = heb_offset + (weights2_3.shape[0]-1)*i + i+j\n","                weights2_3[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o1[i] * o2[j]\n","                                                           + heb_coeffs[idx][1] * o1[i] \n","                                                           + heb_coeffs[idx][2]         * o2[j]  + heb_coeffs[idx][4])\n","\n","        heb_offset += weights2_3.shape[1] * weights2_3.shape[0]\n","        # Layer 3\n","        for i in range(weights3_4.shape[1]): \n","            for j in range(weights3_4.shape[0]): \n","                idx = heb_offset + (weights3_4.shape[0]-1)*i + i+j \n","                weights3_4[:,i][j] += heb_coeffs[idx][3] * ( heb_coeffs[idx][0] * o2[i] * o3[j]\n","                                                           + heb_coeffs[idx][1] * o2[i] \n","                                                           + heb_coeffs[idx][2]         * o3[j]  + heb_coeffs[idx][4])\n","                \n","        return weights1_2, weights2_3, weights3_4"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:04:29.788571Z","iopub.status.busy":"2023-04-30T15:04:29.788078Z","iopub.status.idle":"2023-04-30T15:04:29.833630Z","shell.execute_reply":"2023-04-30T15:04:29.832213Z","shell.execute_reply.started":"2023-04-30T15:04:29.788510Z"},"id":"20e9f4fa-0689-4463-9fdf-48908932a18b","trusted":true},"outputs":[],"source":["class CartWorldMeta():\n","    def __init__(self,env=env,agent=None):\n","        self.env=env\n","        self.test_episodes=[]\n","        self.world_over=False\n","        self.agent = agent\n","    def stop(self):\n","        self.world_over=True\n","    def run_all(self,perf,n_episodes=10,total_tasks=None,replay=None):\n","        if total_tasks is None:\n","            for g in perf:\n","                for l in perf[g]:\n","                    self.env.length,self.env.gravity=l,g\n","                    self.env.polemass_length = self.env.masspole * self.env.length\n","                    self.run(n_episodes=n_episodes)\n","            return []\n","        elif replay is None:\n","            replay=[]\n","            for task in range(total_tasks):\n","                env.length=random.choice(list(perf[10].keys()))\n","                env.gravity=random.choice(list(perf.keys()))\n","                replay+=[(env.length,env.gravity)]\n","                env.polemass_length = env.masspole * env.length\n","                print(f'Task:{task},length:{env.length},gravity:{env.gravity}')\n","                self.run(n_episodes=n_episodes)\n","            return replay\n","        else:\n","            for l,g in replay:\n","                env.length=l\n","                env.gravity=g\n","                env.polemass_length = env.masspole * env.length\n","                print(f'length:{env.length},gravity:{env.gravity}')\n","                self.run(n_episodes=n_episodes)\n","            return replay\n","        \n","    def get_population(self,coevolved_param = False): \n","        \n","        population = []\n","            \n","        if coevolved_param == False:\n","            for i in range( int(self.agent.args['popsize']/2) ):\n","                x = []\n","                x2 = []\n","                for w in coeffs:\n","                    j = np.random.randn(*w.shape)             # j: (coefficients_per_synapse, 1) eg. (5,1)\n","                    x.append(j)                                                   # x: (coefficients_per_synapse, number of synapses) eg. (92690, 5)\n","                    x2.append(-j) \n","                population.append(x)                                              # population : (population size, coefficients_per_synapse, number of synapses), eg. (10, 92690, 5)\n","                population.append(x2)\n","                \n","        return np.array(population).astype(np.float32)\n","    def worker_process_hebb(self,arg):\n","        get_reward_func, hebb_rule,  environment,  init_weights, coeffs, episode_maxlen = arg\n","        \n","        wp = np.array(coeffs)\n","        decay = - 0.01 * np.mean(wp**2)\n","        r = get_reward_func( hebb_rule,  environment,  init_weights, coeffs,episode_maxlen) + decay\n","        \n","        return r\n","    \n","    def _get_params_try(self, w, p):\n","\n","        param_try = []\n","        for index, i in enumerate(p):\n","            jittered = self.agent.args['sigma'] * i\n","            param_try.append(w[index] + jittered)\n","        param_try = np.array(param_try).astype(np.float32)\n","        \n","        return param_try\n","    \n","\n","    def get_worker_args(self,pool , population, episode_maxlen):\n","\n","        rewards = []\n","        for p in population:\n","            heb_coeffs_try = np.array(self._get_params_try(coeffs, p))\n","            rewards.append(self.fitness( self.agent.args['hebb_rule'], self.agent.args['environment'],  self.agent.args['init_weights'], heb_coeffs_try))\n","        \n","        rewards = np.array(rewards).astype(np.float32)\n","\n","        return rewards\n","    \n","    def fitness(self,hebb_rule : str, environment : str, init_weights = 'uni' , *evolved_parameters: List[np.array], episode_maxlen = 100) -> float:\n","        def weights_init(m):\n","            if isinstance(m, torch.nn.Linear):\n","                if init_weights == 'xa_uni':  \n","                    torch.nn.init.xavier_uniform(m.weight.data, 0.3)\n","                elif init_weights == 'sparse':  \n","                    torch.nn.init.sparse_(m.weight.data, 0.8)\n","                elif init_weights == 'uni':  \n","                    torch.nn.init.uniform_(m.weight.data, -0.1, 0.1)\n","                elif init_weights == 'normal':  \n","                    torch.nn.init.normal_(m.weight.data, 0, 0.024)\n","                elif init_weights == 'ka_uni':  \n","                    torch.nn.init.kaiming_uniform_(m.weight.data, 3)\n","                elif init_weights == 'uni_big':\n","                    torch.nn.init.uniform_(m.weight.data, -1, 1)\n","                elif init_weights == 'xa_uni_big':\n","                    torch.nn.init.xavier_uniform(m.weight.data)\n","                elif init_weights == 'ones':\n","                    torch.nn.init.ones_(m.weight.data)\n","                elif init_weights == 'zeros':\n","                    torch.nn.init.zeros_(m.weight.data)\n","                elif init_weights == 'default':\n","                    pass\n","\n","        # Unpack evolved parameters\n","        try: \n","            hebb_coeffs, initial_weights_co = evolved_parameters\n","        except: \n","            hebb_coeffs = evolved_parameters[0]\n","        with torch.no_grad():\n","            \n","            # decide whether to use initiase a new env or use the old env\n","\n","            if len(env.observation_space.shape) == 1:   \n","                input_dim = env.observation_space.shape[0]\n","            elif len(env.observation_space.shape) == 0:   \n","                input_dim = env.observation_space.n\n","            \n","            \n","\n","                \n","            # Determine action space dimension\n","            if isinstance(env.action_space, Discrete):\n","                action_dim = env.action_space.n\n","            else:\n","                raise ValueError('Only Box and Discrete action spaces supported')\n","            \n","            p = MLP_heb(input_dim, action_dim)\n","            p.apply(weights_init)\n","            p = p.float()\n","            weights1_2, weights2_3, weights3_4 = list(p.parameters())\n","            weights1_2 = weights1_2.detach().numpy()\n","            weights2_3 = weights2_3.detach().numpy()\n","            weights3_4 = weights3_4.detach().numpy()\n","\n","            observation = self.env.reset()\n","            self.agent.begin()\n","\n","            rew_eq = 0\n","            from tqdm import tqdm\n","            # for t in (range(episode_maxlen)):\n","            while True:\n","                o0, o1, o2, o3 = p([observation])\n","                o0 = o0.numpy()\n","                o1 = o1.numpy()\n","                o2 = o2.numpy()\n","                action = self.agent.act(o3)\n","                o3 = o3.numpy()\n","                observation, reward, done, info = self.env.step(action)\n","                # print(\"rew : \", reward)\n","                rew_eq += reward\n","                self.agent.reward((reward, done, info))\n","\n","                if done:\n","                    break\n","                weights1_2, weights2_3, weights3_4 = hebbian_update_ABCD_lr_D_in(hebb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3)\n","                \n","                (a, b, c) = (0, 1, 2)\n","                list(p.parameters())[a].data /= list(p.parameters())[a].__abs__().max()\n","                list(p.parameters())[b].data /= list(p.parameters())[b].__abs__().max()\n","                list(p.parameters())[c].data /= list(p.parameters())[c].__abs__().max()\n","            env.close()\n","        return rew_eq\n","\n","\n","    def run(self,n_episodes=10,episode_maxlen=1000):\n","        self.agent.observation_space=env.observation_space\n","        if 'training' not in self.agent.__dict__: self.agent.training=False\n","        if self.agent.training: testing=False \n","        else: testing=True\n","        if self.agent.training: print('Starting Training time: ',self.agent.time)\n","        for episode in range(n_episodes):\n","            generation_rewards = []\n","            pool = mp.Pool(num_threads) if num_threads > 1 else None\n","            for generation in range(self.agent.args['generations']):\n","                population = self.get_population()\n","                rewards = self.get_worker_args(pool, population, episode_maxlen)\n","                print(rewards)\n","                self.agent._update_coeffs(rewards, population)\n","#                 self.agent.reward((rewards, population))\n","                \n","\n","\n","            # print('CartAgent','starting episode')\n","            # state=self.env.reset()\n","            # self.agent.begin()\n","            # # print(agent.time)#,agent.ep)\n","            # for t in range(episode_maxlen):\n","            #     # print(f'length:{env.length},gravity:{env.gravity}')\n","            #     # env.render(mode='rgb_array')\n","            #     action=self.agent.act(state)\n","            #     # print(episode,t,'Action: ', action)\n","            #     state, reward, done, info = env.step(action)\n","            #     self.agent.reward((reward,done,info))\n","            #     # print(episode,t,'Reward sent: ', reward)\n","            #     if done:\n","            #         break\n","            if self.world_over:break\n","            if not self.agent.training: self.test_episodes+=[episode]\n","            if not self.agent.training and not testing: \n","                print('Training Over at time: ',self.agent.time)\n","                testing=True\n","                self.world_over=True\n","        print('Testing Done time: ', self.agent.time, ' Reward: ', self.agent.avg_rew())\n","        return self.agent.avg_rew()"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:04:29.980017Z","iopub.status.busy":"2023-04-30T15:04:29.979567Z","iopub.status.idle":"2023-04-30T15:04:30.000459Z","shell.execute_reply":"2023-04-30T15:04:29.999047Z","shell.execute_reply.started":"2023-04-30T15:04:29.979978Z"},"trusted":true},"outputs":[],"source":["class HebbianAgent(AIAgent):\n","    def __init__(self, args):\n","        super().__init__()\n","        self.actor=self.Actor(parent=self)\n","        self.tot_rew=0\n","        # self.mlp_heb = MLP_heb(4, 2)\n","        self.rewL=[]\n","        self.args = args\n","        \n","    class Actor(Actor):\n","        def __init__(self,parent): \n","            super().__init__(parent=parent)\n","\n","        def call_model(self,observation):\n","            action = np.argmax(observation).numpy()\n","            return action\n","            \n","        def compute_reward(self,reward):\n","            return reward[0]\n","    \n","    def reward(self,rew):\n","#         print(rew)\n","#         # learning_rate = self.args[\"learning_rate\"]\n","#         # sigma = self.args[\"sigma\"]\n","#         # decay = self.args[\"decay\"]\n","# #         print(\"Initial rew \", rew, type(rew))\n","#         rewards = rew[0]\n","#         population = rew[1]\n","        \n","#         rewards = self.compute_centered_ranks(rewards)\n","#         std = rewards.std()\n","#         if std == 0:\n","#             raise ValueError('Variance should not be zero')\n","        \n","#         rewards = (rewards - rewards.mean()) / std\n","# #         print(\"rewards :\", rewards)\n","#         for index, c in enumerate(coeffs):\n","#             layer_population = np.array([p[index] for p in population])\n","#             update_factor = self.agent.args['learning_rate'] / (self.agent.args['popsize'] * self.agent.args['sigma'])\n","                      \n","#             coeffs[index] = c + update_factor * np.dot(layer_population.T, rewards).T \n","\n","#         if(self.agent.args['learning_rate'] > 0.001):\n","#             self.agent.args['learning_rate'] *= self.args[\"decay\"]\n","\n","#         if(self.args[\"sigma\"] > 0.001):\n","#             self.args[\"sigma\"] *= 0.999\n","  \n","#         ##Augmenting AIAgent\n","        self.tot_rew+=rew[0]\n","# #         print(rew[0].mean())\n","\n","\n","        return super().reward(rew)\n","    \n","    def _update_coeffs(self, rewards, population):\n","            rewards = self.compute_centered_ranks(rewards)\n","            std = rewards.std()\n","            if std == 0:\n","                raise ValueError('Variance should not be zero')\n","            \n","            rewards = (rewards - rewards.mean()) / std\n","    #         print(\"rewards :\", rewards)\n","            for index, c in enumerate(coeffs):\n","                layer_population = np.array([p[index] for p in population])\n","                update_factor = self.args['learning_rate'] / (self.args['popsize'] * self.args['sigma'])\n","                        \n","                coeffs[index] = c + update_factor * np.dot(layer_population.T, rewards).T \n","\n","            if(self.args['learning_rate'] > 0.001):\n","                self.args['learning_rate'] *= self.args[\"decay\"]\n","\n","            if(self.args[\"sigma\"] > 0.001):\n","                self.args[\"sigma\"] *= 0.999\n","\n","    def compute_ranks(self,x):\n","        \"\"\"\n","        Returns rank as a vector of len(x) with integers from 0 to len(x)\n","        \"\"\"\n","        assert x.ndim == 1\n","        ranks = np.empty(len(x), dtype=int)\n","        ranks[x.argsort()] = np.arange(len(x))\n","        return ranks\n","    def compute_centered_ranks(self,x):\n","        \"\"\"\n","        Maps x to [-0.5, 0.5] and returns the rank\n","        \"\"\"\n","        y = self.compute_ranks(x.ravel()).reshape(x.shape).astype(np.float32)\n","        y /= (x.size - 1)\n","        y -= .5\n","        return y\n","    def begin(self):\n","        ##Augmenting AIAgent\n","        self.rewL+=[self.tot_rew]\n","        super().begin()\n","    def avg_rew(self):\n","        return sum(self.rewL)/len(self.rewL)"]},{"cell_type":"code","execution_count":119,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:04:30.175776Z","iopub.status.busy":"2023-04-30T15:04:30.175316Z","iopub.status.idle":"2023-04-30T15:04:30.186218Z","shell.execute_reply":"2023-04-30T15:04:30.184881Z","shell.execute_reply.started":"2023-04-30T15:04:30.175739Z"},"id":"9a95733e-f863-432b-ac20-753d221e6fa9","trusted":true},"outputs":[],"source":["class RandomAIAgent(AIAgent):\n","    def __init__(self,action_space):\n","        super().__init__()\n","        self.actor=self.Actor(parent=self)\n","        self.action_space=action_space\n","        self.tot_rew=0\n","        self.rewL=[]\n","        \n","    class Actor(Actor):\n","        def __init__(self,parent): \n","            super().__init__(parent=parent)\n","        def call_model(self,state):\n","        ##Overriding AIAgent.Model\n","            # print(self.parent.action_space.sample())\n","            action = self.parent.action_space.sample()\n","            return action\n","        def compute_reward(self,reward):\n","            return reward[0]\n","    \n","    def reward(self,rew):\n","        ##Augmenting AIAgent\n","        self.tot_rew+=rew[0]\n","        print(\"reward : \", rew, type(rew))\n","        return super().reward(rew)\n","    def begin(self):\n","        ##Augmenting AIAgent\n","        self.rewL+=[self.tot_rew]\n","        super().begin()\n","    def avg_rew(self):\n","        return sum(self.rewL)/len(self.rewL)"]},{"cell_type":"code","execution_count":120,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:04:30.348391Z","iopub.status.busy":"2023-04-30T15:04:30.347463Z","iopub.status.idle":"2023-04-30T15:04:30.353942Z","shell.execute_reply":"2023-04-30T15:04:30.352728Z","shell.execute_reply.started":"2023-04-30T15:04:30.348348Z"},"id":"b122fb6f-f23c-41ba-95b4-728b058b88c0","trusted":true},"outputs":[],"source":["import pickle\n","with open('./perf.pickle','rb') as f: perf=pickle.load(f)"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:04:30.584421Z","iopub.status.busy":"2023-04-30T15:04:30.583867Z","iopub.status.idle":"2023-04-30T15:04:30.591623Z","shell.execute_reply":"2023-04-30T15:04:30.589760Z","shell.execute_reply.started":"2023-04-30T15:04:30.584381Z"},"id":"d01edb66-b700-4f8b-a922-069b1de00c49","trusted":true},"outputs":[],"source":["agent=HebbianAgent(args)\n"]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:04:31.081928Z","iopub.status.busy":"2023-04-30T15:04:31.080153Z","iopub.status.idle":"2023-04-30T15:04:31.091344Z","shell.execute_reply":"2023-04-30T15:04:31.089026Z","shell.execute_reply.started":"2023-04-30T15:04:31.081862Z"},"trusted":true},"outputs":[],"source":["env = CartPoleEnv()\n"]},{"cell_type":"code","execution_count":123,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:04:31.297796Z","iopub.status.busy":"2023-04-30T15:04:31.297227Z","iopub.status.idle":"2023-04-30T15:04:31.304517Z","shell.execute_reply":"2023-04-30T15:04:31.303262Z","shell.execute_reply.started":"2023-04-30T15:04:31.297751Z"},"id":"9712783c-fd5a-420a-9b27-91f2c5a28e68","trusted":true},"outputs":[],"source":["agent.training=True\n","agent.debug=False\n","agent.use_memory=True\n","agent.limit_memory=True\n","agent.memory.limit_perceptual=2\n","agent.memory.limit_sar=4"]},{"cell_type":"code","execution_count":124,"metadata":{"execution":{"iopub.execute_input":"2023-04-30T15:04:31.725625Z","iopub.status.busy":"2023-04-30T15:04:31.725168Z","iopub.status.idle":"2023-04-30T15:04:31.732947Z","shell.execute_reply":"2023-04-30T15:04:31.731343Z","shell.execute_reply.started":"2023-04-30T15:04:31.725584Z"},"id":"c43d9c18-d92e-478a-8eff-dd68f0af3d10","trusted":true},"outputs":[],"source":["world=CartWorldMeta(env=env, agent=agent)"]},{"cell_type":"code","execution_count":125,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"execution":{"iopub.execute_input":"2023-04-30T15:04:32.024386Z","iopub.status.busy":"2023-04-30T15:04:32.023557Z","iopub.status.idle":"2023-04-30T15:04:32.846108Z","shell.execute_reply":"2023-04-30T15:04:32.844065Z","shell.execute_reply.started":"2023-04-30T15:04:32.024345Z"},"id":"b8644cbd-6139-491d-8f12-720a4b619e07","outputId":"75ec36ad-d91b-4b35-ca21-735f555d8a63","tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Task:0,length:4.25,gravity:140\n","Starting Training time:  0\n","[35. 30. 28. 55. 42. 41. 48. 36. 52. 38.]\n","[32. 27. 57. 21. 18. 19. 22. 23. 28. 44.]\n","[49. 40. 31. 36. 30. 67. 28. 29. 21. 34.]\n","[20. 22. 36. 20. 19. 24. 22. 36. 20. 35.]\n","[33. 27. 35. 29. 35. 46. 64. 30. 30. 32.]\n","[31. 25. 31. 29. 48. 21. 39. 26. 19. 24.]\n","[32. 26. 23. 33. 33. 36. 37. 19. 25. 20.]\n","[20. 27. 22. 20. 43. 25. 37. 27. 39. 20.]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[125], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r\u001b[39m=\u001b[39mworld\u001b[39m.\u001b[39;49mrun_all(perf,total_tasks\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,replay\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n","Cell \u001b[0;32mIn[117], line 25\u001b[0m, in \u001b[0;36mCartWorldMeta.run_all\u001b[0;34m(self, perf, n_episodes, total_tasks, replay)\u001b[0m\n\u001b[1;32m     23\u001b[0m         env\u001b[39m.\u001b[39mpolemass_length \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mmasspole \u001b[39m*\u001b[39m env\u001b[39m.\u001b[39mlength\n\u001b[1;32m     24\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTask:\u001b[39m\u001b[39m{\u001b[39;00mtask\u001b[39m}\u001b[39;00m\u001b[39m,length:\u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39mlength\u001b[39m}\u001b[39;00m\u001b[39m,gravity:\u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39mgravity\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(n_episodes\u001b[39m=\u001b[39;49mn_episodes)\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m replay\n\u001b[1;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n","Cell \u001b[0;32mIn[117], line 179\u001b[0m, in \u001b[0;36mCartWorldMeta.run\u001b[0;34m(self, n_episodes, episode_maxlen)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mgenerations\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    178\u001b[0m     population \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_population()\n\u001b[0;32m--> 179\u001b[0m     rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_worker_args(pool, population, episode_maxlen)\n\u001b[1;32m    180\u001b[0m     \u001b[39mprint\u001b[39m(rewards)\n\u001b[1;32m    181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39m_update_coeffs(rewards, population)\n","Cell \u001b[0;32mIn[117], line 77\u001b[0m, in \u001b[0;36mCartWorldMeta.get_worker_args\u001b[0;34m(self, pool, population, episode_maxlen)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m population:\n\u001b[1;32m     76\u001b[0m     heb_coeffs_try \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_params_try(coeffs, p))\n\u001b[0;32m---> 77\u001b[0m     rewards\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitness( \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49margs[\u001b[39m'\u001b[39;49m\u001b[39mhebb_rule\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49margs[\u001b[39m'\u001b[39;49m\u001b[39menvironment\u001b[39;49m\u001b[39m'\u001b[39;49m],  \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49margs[\u001b[39m'\u001b[39;49m\u001b[39minit_weights\u001b[39;49m\u001b[39m'\u001b[39;49m], heb_coeffs_try))\n\u001b[1;32m     79\u001b[0m rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(rewards)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m rewards\n","Cell \u001b[0;32mIn[117], line 158\u001b[0m, in \u001b[0;36mCartWorldMeta.fitness\u001b[0;34m(self, hebb_rule, environment, init_weights, episode_maxlen, *evolved_parameters)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m    157\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m weights1_2, weights2_3, weights3_4 \u001b[39m=\u001b[39m hebbian_update_ABCD_lr_D_in(hebb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3)\n\u001b[1;32m    160\u001b[0m (a, b, c) \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[39mlist\u001b[39m(p\u001b[39m.\u001b[39mparameters())[a]\u001b[39m.\u001b[39mdata \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(p\u001b[39m.\u001b[39mparameters())[a]\u001b[39m.\u001b[39m\u001b[39m__abs__\u001b[39m()\u001b[39m.\u001b[39mmax()\n","Cell \u001b[0;32mIn[82], line 19\u001b[0m, in \u001b[0;36mhebbian_update_ABCD_lr_D_in\u001b[0;34m(heb_coeffs, weights1_2, weights2_3, weights3_4, o0, o1, o2, o3)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(weights2_3\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     16\u001b[0m         idx \u001b[39m=\u001b[39m heb_offset \u001b[39m+\u001b[39m (weights2_3\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mi \u001b[39m+\u001b[39m i\u001b[39m+\u001b[39mj\n\u001b[1;32m     17\u001b[0m         weights2_3[:,i][j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m heb_coeffs[idx][\u001b[39m3\u001b[39m] \u001b[39m*\u001b[39m ( heb_coeffs[idx][\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m o1[i] \u001b[39m*\u001b[39m o2[j]\n\u001b[1;32m     18\u001b[0m                                                    \u001b[39m+\u001b[39m heb_coeffs[idx][\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m o1[i] \n\u001b[0;32m---> 19\u001b[0m                                                    \u001b[39m+\u001b[39m heb_coeffs[idx][\u001b[39m2\u001b[39;49m]         \u001b[39m*\u001b[39;49m o2[j]  \u001b[39m+\u001b[39m heb_coeffs[idx][\u001b[39m4\u001b[39m])\n\u001b[1;32m     21\u001b[0m heb_offset \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m weights2_3\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m weights2_3\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[39m# Layer 3\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["r=world.run_all(perf,total_tasks=20,replay=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-04-30T15:02:57.128040Z","iopub.status.idle":"2023-04-30T15:02:57.129020Z","shell.execute_reply":"2023-04-30T15:02:57.128607Z","shell.execute_reply.started":"2023-04-30T15:02:57.128559Z"},"id":"b9a3b05a-3899-41c8-9c67-b73722da84af","outputId":"b35d8cfb-c6f7-4ec5-8ca1-69f4d1839803","trusted":true},"outputs":[],"source":["agent.avg_rew()/len(agent.ep)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.132628Z","iopub.status.idle":"2023-04-30T15:02:57.133844Z","shell.execute_reply":"2023-04-30T15:02:57.133423Z","shell.execute_reply.started":"2023-04-30T15:02:57.133379Z"},"id":"45180a22-fbfb-4982-b123-dd2ddbbe84d9","tags":[],"trusted":true},"outputs":[],"source":["# agent.memory.perceptual_memory"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.136523Z","iopub.status.idle":"2023-04-30T15:02:57.137882Z","shell.execute_reply":"2023-04-30T15:02:57.137479Z","shell.execute_reply.started":"2023-04-30T15:02:57.137427Z"},"id":"68549844-fcd7-4396-8669-34d145eb7e17","trusted":true},"outputs":[],"source":["with open('./replay.pickle','wb') as f: pickle.dump(r,f)"]},{"cell_type":"markdown","metadata":{"id":"41e24475-66fc-4544-a0fe-20297b9246ac"},"source":["### Training an AI Agent's Model using Generic RL Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.140323Z","iopub.status.idle":"2023-04-30T15:02:57.141765Z","shell.execute_reply":"2023-04-30T15:02:57.141369Z","shell.execute_reply.started":"2023-04-30T15:02:57.141317Z"},"id":"afefe229-241d-42fb-b5a2-0fe4b4f36c28","trusted":true},"outputs":[],"source":["from threading import Thread\n","import threading\n","import sys"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.143488Z","iopub.status.idle":"2023-04-30T15:02:57.145459Z","shell.execute_reply":"2023-04-30T15:02:57.145030Z","shell.execute_reply.started":"2023-04-30T15:02:57.144959Z"},"id":"52cd9f07-62c7-4d48-af53-641fbe1ee17c","trusted":true},"outputs":[],"source":["from queue import Queue"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.148231Z","iopub.status.idle":"2023-04-30T15:02:57.148952Z","shell.execute_reply":"2023-04-30T15:02:57.148645Z","shell.execute_reply.started":"2023-04-30T15:02:57.148610Z"},"id":"cae673aa-2a03-4f7e-b067-e22eb58846cc","trusted":true},"outputs":[],"source":["from aiagentbase import RLAgent"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.150730Z","iopub.status.idle":"2023-04-30T15:02:57.151333Z","shell.execute_reply":"2023-04-30T15:02:57.151060Z","shell.execute_reply.started":"2023-04-30T15:02:57.151028Z"},"id":"7243708f-daf6-4305-a0b0-11fa82153781","trusted":true},"outputs":[],"source":["training_steps=1000000"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.153445Z","iopub.status.idle":"2023-04-30T15:02:57.154079Z","shell.execute_reply":"2023-04-30T15:02:57.153806Z","shell.execute_reply.started":"2023-04-30T15:02:57.153772Z"},"id":"7f938dfc-36bd-4e18-87c0-584c3c8c0f80","trusted":true},"outputs":[],"source":["agent=RLAgent(algoclass=PPO,monclass=Mon,action_space=env.action_space,observation_space=env.observation_space,\n","              verbose=0,metarl=True,win=10,soclass=StackedObservations)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.157428Z","iopub.status.idle":"2023-04-30T15:02:57.158502Z","shell.execute_reply":"2023-04-30T15:02:57.158196Z","shell.execute_reply.started":"2023-04-30T15:02:57.158162Z"},"id":"44edef18-00b7-4098-96f8-01f51bccc195","trusted":true},"outputs":[],"source":["agent.debug=False\n","agent.use_memory=True\n","agent.training=True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.161300Z","iopub.status.idle":"2023-04-30T15:02:57.161975Z","shell.execute_reply":"2023-04-30T15:02:57.161678Z","shell.execute_reply.started":"2023-04-30T15:02:57.161645Z"},"id":"d1cce67f-0995-42fa-b1a1-1753133a37f6","trusted":true},"outputs":[],"source":["agent.rewL=[]\n","agent.tot_rew=0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.163901Z","iopub.status.idle":"2023-04-30T15:02:57.164562Z","shell.execute_reply":"2023-04-30T15:02:57.164240Z","shell.execute_reply.started":"2023-04-30T15:02:57.164207Z"},"id":"e6fd4736-40ee-4be8-9d65-95804512b2d3","trusted":true},"outputs":[],"source":["agent.start(training_steps=training_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.166709Z","iopub.status.idle":"2023-04-30T15:02:57.167322Z","shell.execute_reply":"2023-04-30T15:02:57.167041Z","shell.execute_reply.started":"2023-04-30T15:02:57.167007Z"},"id":"bb1e7d8e-eae8-43f4-98b2-28f652813c17","trusted":true},"outputs":[],"source":["world=CartWorldMeta(env=env)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.169281Z","iopub.status.idle":"2023-04-30T15:02:57.169929Z","shell.execute_reply":"2023-04-30T15:02:57.169643Z","shell.execute_reply.started":"2023-04-30T15:02:57.169609Z"},"id":"825ee6db-c404-4453-a405-c2cf8c54804b","trusted":true},"outputs":[],"source":["# worldthread=Thread(name='world',target=world.run,args=(agent,2000,200))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.171868Z","iopub.status.idle":"2023-04-30T15:02:57.172492Z","shell.execute_reply":"2023-04-30T15:02:57.172201Z","shell.execute_reply.started":"2023-04-30T15:02:57.172170Z"},"id":"c295f9e2-c817-46d1-9078-c2135216f80b","tags":[],"trusted":true},"outputs":[],"source":["# worldthread.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.173998Z","iopub.status.idle":"2023-04-30T15:02:57.174617Z","shell.execute_reply":"2023-04-30T15:02:57.174307Z","shell.execute_reply.started":"2023-04-30T15:02:57.174276Z"},"id":"54061a8f-0775-42ce-a95f-300931b2eb4f","tags":[],"trusted":true},"outputs":[],"source":["# len(agent.logL)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.177349Z","iopub.status.idle":"2023-04-30T15:02:57.178001Z","shell.execute_reply":"2023-04-30T15:02:57.177706Z","shell.execute_reply.started":"2023-04-30T15:02:57.177674Z"},"id":"56c2177c-3cdc-4dde-b113-606190837ff3","trusted":true},"outputs":[],"source":["# agent.time"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.180136Z","iopub.status.idle":"2023-04-30T15:02:57.180760Z","shell.execute_reply":"2023-04-30T15:02:57.180449Z","shell.execute_reply.started":"2023-04-30T15:02:57.180417Z"},"id":"077d1ab6-3a4e-4f75-a38a-0236a585f709","trusted":true},"outputs":[],"source":["# agent.memory.sar_memory[10011]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.183155Z","iopub.status.idle":"2023-04-30T15:02:57.183807Z","shell.execute_reply":"2023-04-30T15:02:57.183497Z","shell.execute_reply.started":"2023-04-30T15:02:57.183455Z"},"id":"97014884-b69f-46e7-8be3-069a59bdf6ef","trusted":true},"outputs":[],"source":["with open('./replay.pickle','rb') as f: replay=pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.186453Z","iopub.status.idle":"2023-04-30T15:02:57.187121Z","shell.execute_reply":"2023-04-30T15:02:57.186830Z","shell.execute_reply.started":"2023-04-30T15:02:57.186796Z"},"id":"2a2eb86f-9e74-44e8-b478-6d0ce518b196","outputId":"ac6e3735-2e96-490e-eaca-18c7a0585167","tags":[],"trusted":true},"outputs":[],"source":["_=world.run_all(agent,perf,n_episodes=50,total_tasks=20,replay=replay)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.189780Z","iopub.status.idle":"2023-04-30T15:02:57.190447Z","shell.execute_reply":"2023-04-30T15:02:57.190136Z","shell.execute_reply.started":"2023-04-30T15:02:57.190102Z"},"id":"0f3125f5-d1b5-4642-bc9f-ccfa258ff799","outputId":"80faba67-5790-4a9f-a73b-3360e7d04f01","trusted":true},"outputs":[],"source":["# Without metarl training\n","agent.avg_rew()/len(agent.ep)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.192707Z","iopub.status.idle":"2023-04-30T15:02:57.193318Z","shell.execute_reply":"2023-04-30T15:02:57.193036Z","shell.execute_reply.started":"2023-04-30T15:02:57.193004Z"},"id":"ed8afd67-3fc9-4fc0-b74d-91df72029dba","outputId":"bdcd6f7c-0524-4af3-ae51-f53ed7d24383","trusted":true},"outputs":[],"source":["# With metarl training\n","agent.avg_rew()/len(agent.ep)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.195087Z","iopub.status.idle":"2023-04-30T15:02:57.195729Z","shell.execute_reply":"2023-04-30T15:02:57.195405Z","shell.execute_reply.started":"2023-04-30T15:02:57.195373Z"},"id":"6dfb1406-99e7-4b80-b0ed-1e4eb8b91e8c","trusted":true},"outputs":[],"source":["agent.training=False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.197890Z","iopub.status.idle":"2023-04-30T15:02:57.198504Z","shell.execute_reply":"2023-04-30T15:02:57.198217Z","shell.execute_reply.started":"2023-04-30T15:02:57.198185Z"},"id":"262e88c7-7798-42c2-b608-9a0765293429","outputId":"6d7b53e5-e1d7-4132-df40-c7d136f73ae9","trusted":true},"outputs":[],"source":["_=world.run_all(agent,perf,n_episodes=50,total_tasks=20,replay=replay)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.200251Z","iopub.status.idle":"2023-04-30T15:02:57.200880Z","shell.execute_reply":"2023-04-30T15:02:57.200604Z","shell.execute_reply.started":"2023-04-30T15:02:57.200569Z"},"id":"17c21089-cdcc-4647-8b40-e7a767e8be3e","outputId":"c19fc705-d617-45dc-cb21-25c011dce1ad","trusted":true},"outputs":[],"source":["# Without metarl testing\n","agent.avg_rew()/len(agent.ep)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.202555Z","iopub.status.idle":"2023-04-30T15:02:57.203140Z","shell.execute_reply":"2023-04-30T15:02:57.202872Z","shell.execute_reply.started":"2023-04-30T15:02:57.202839Z"},"id":"9346ed05-cf89-43e5-886c-098b32d74b43","outputId":"ec2a21ba-8972-4785-e6e9-8e1ecc644907","trusted":true},"outputs":[],"source":["# With metarl testing\n","agent.avg_rew()/len(agent.ep)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.204759Z","iopub.status.idle":"2023-04-30T15:02:57.205336Z","shell.execute_reply":"2023-04-30T15:02:57.205069Z","shell.execute_reply.started":"2023-04-30T15:02:57.205038Z"},"id":"04b4a528-95bd-46df-89b5-24208287a20f","trusted":true},"outputs":[],"source":["from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.208173Z","iopub.status.idle":"2023-04-30T15:02:57.208829Z","shell.execute_reply":"2023-04-30T15:02:57.208507Z","shell.execute_reply.started":"2023-04-30T15:02:57.208464Z"},"id":"ed44ee84-a7ee-4c78-a177-21f84f5243bf","trusted":true},"outputs":[],"source":["# testing_len=len([agent.rewL[t] for t in world.test_episodes])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.210457Z","iopub.status.idle":"2023-04-30T15:02:57.211071Z","shell.execute_reply":"2023-04-30T15:02:57.210803Z","shell.execute_reply.started":"2023-04-30T15:02:57.210771Z"},"id":"e554e4c8-2681-4bd6-b805-b708fa863ab4","trusted":true},"outputs":[],"source":["# testing_len"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.212932Z","iopub.status.idle":"2023-04-30T15:02:57.213522Z","shell.execute_reply":"2023-04-30T15:02:57.213247Z","shell.execute_reply.started":"2023-04-30T15:02:57.213216Z"},"id":"6f1c83e7-4355-453e-84f0-fb8df19e141f","tags":[],"trusted":true},"outputs":[],"source":["# agent.rewL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.215680Z","iopub.status.idle":"2023-04-30T15:02:57.216277Z","shell.execute_reply":"2023-04-30T15:02:57.216002Z","shell.execute_reply.started":"2023-04-30T15:02:57.215970Z"},"id":"b24174f7-ddec-495e-bd52-9612889571c2","trusted":true},"outputs":[],"source":["# print(np.gradient(agent.rewL).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.220840Z","iopub.status.idle":"2023-04-30T15:02:57.221477Z","shell.execute_reply":"2023-04-30T15:02:57.221224Z","shell.execute_reply.started":"2023-04-30T15:02:57.221196Z"},"id":"6688ab07-4271-4eb0-8cd4-11876c225adb","outputId":"233ec98a-0dff-4ffe-c4de-0192bf42b8d6","trusted":true},"outputs":[],"source":["# Without metarl \n","plt.plot(np.gradient(agent.rewL))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.223142Z","iopub.status.idle":"2023-04-30T15:02:57.223735Z","shell.execute_reply":"2023-04-30T15:02:57.223485Z","shell.execute_reply.started":"2023-04-30T15:02:57.223457Z"},"id":"8ff5dfe8-8654-4ae0-b155-dbfc156f5a6c","outputId":"41f94d47-7a2b-4936-cd39-5642c08c5fc4","trusted":true},"outputs":[],"source":["# With metarl \n","plt.plot(np.gradient(agent.rewL))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.226225Z","iopub.status.idle":"2023-04-30T15:02:57.226855Z","shell.execute_reply":"2023-04-30T15:02:57.226604Z","shell.execute_reply.started":"2023-04-30T15:02:57.226574Z"},"id":"31b82967-748a-4909-9228-8d2de329388e","trusted":true},"outputs":[],"source":["episodes = 50\n","rewL=[]\n","for episode in range(1, episodes+1):\n","    state = env.reset()\n","    done = False\n","    score = 0 \n","    steps=0\n","    while not done and steps<=200:\n","        env.render()\n","        action=agent.act(state)\n","        # action,_ = agent.model.predict(state)\n","        state, reward, done, info = env.step(action)\n","        score+=reward\n","        steps+=1\n","    # print('Episode:{} Score:{}'.format(episode, score))\n","    rewL+=[score]\n","env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.229254Z","iopub.status.idle":"2023-04-30T15:02:57.229915Z","shell.execute_reply":"2023-04-30T15:02:57.229650Z","shell.execute_reply.started":"2023-04-30T15:02:57.229621Z"},"id":"30e78bfe-2dda-4fe5-a39d-06c88022a710","trusted":true},"outputs":[],"source":["print(np.array(rewL).mean())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-30T15:02:57.232283Z","iopub.status.idle":"2023-04-30T15:02:57.232859Z","shell.execute_reply":"2023-04-30T15:02:57.232616Z","shell.execute_reply.started":"2023-04-30T15:02:57.232591Z"},"id":"81793756-e0ca-43b1-b3b5-a4a74c9083fd","trusted":true},"outputs":[],"source":["plt.plot(rewL)"]},{"cell_type":"markdown","metadata":{"id":"d7efbcb2-fe2c-498a-bd73-60ef0ec45112"},"source":["###### "]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}
